from dotenv import load_dotenv
import os
from pprint import pprint

from langchain_openai import OpenAIEmbeddings
# from langchain_community.embeddings import DeepSeekEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings

from langchain_ollama import OllamaEmbeddings

from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone, ServerlessSpec

from splitter import split_docs_by_markdown_headers
from loader import load_docs

def embedding():
    load_dotenv()

    deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")

    pinecone_api_key = os.getenv("PINECONE_API_KEY")
    print("PINECONE_API_KEY:", pinecone_api_key)
    pc = Pinecone(api_key=pinecone_api_key)

    # åˆ›å»º Pinecone indexï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    # index_name = "my-md-rag-index"
    # if index_name not in pc.list_indexes().names():
    #     pc.create_index(
    #         name=index_name,
    #         dimension=1536, 
    #         metric="cosine",
    #         spec=ServerlessSpec(
    #             cloud="aws",
    #             region="us-east-1"
    #         )
    #     )
    index_name = "dochelper"
    index = pc.Index(index_name)

    # 1. åŠ è½½æ–‡æ¡£
    root = "D:/GitHub_Repos/InterviewAndLeetCode/é¢è¯•é¢˜"
    docs = load_docs(root)

    # 2. æŒ‰ Markdown æ ‡é¢˜å±‚çº§åˆ‡åˆ†æ–‡æ¡£
    chunks = split_docs_by_markdown_headers(docs)
    print(f"âœ… æŒ‰æ ‡é¢˜åˆ‡åˆ†å®Œæˆï¼Œå…± {len(chunks)} ä¸ªå—")

    # ç¡®å®šä¸€ä¸‹æ¯ä¸ªchunké‡ŒåŸæ–‡å†…å®¹çš„å­—æ®µå«ä»€ä¹ˆï¼Œåé¢embeddingå’Œå‘é‡åº“è¦ç”¨åˆ°
    # pprint(chunks[0].dict())
    # print(type(chunks[0]))

    # 3. åˆ›å»ºå‘é‡embeddingæ¨¡å‹

    # åªèƒ½è¿äº†openaiçš„api keyæ‰èƒ½ç›´æ¥ç”¨
    # embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

    # DeepseekæŸ¥ä¸åˆ°ç¡®åˆ‡çš„embeddingæ¨¡å‹å’Œå‘é‡ç»´åº¦
    # embeddings = OpenAIEmbeddings(
    #     model="text-embedding",                # DeepSeek çš„ embedding æ¨¡å‹å
    #     api_key=deepseek_api_key,
    #     base_url="https://api.deepseek.com"    # å¿…é¡»è®¾ç½®ä¸º DeepSeek åŸŸåï¼
    # )


    """
    æ”¹ç”¨HuggingFacesä¸Šçš„ åƒé—® embeddingæ¨¡å‹

    åƒé—®7Bç‰ˆæœ¬å†…å­˜å ç”¨å¤ªå¤§ï¼Œè¦30ä¸ªGäº†
    model_name = "Alibaba-NLP/gte-Qwen2-7B-instruct"

    1.5Bç‰ˆæœ¬ç›¸å¯¹å°å¾ˆå¤šï¼Œ4-8Gæ˜¾å­˜å°±èƒ½è·‘ä½†æ˜¯ä½¿ç”¨ä¼šç°ä¸‹è½½ï¼Œä¹Ÿå¾ˆæ…¢ï¼Œæ‰€ä»¥ä¸‹é¢ä½¿ç”¨äº‹å…ˆä¸‹å¥½çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
    model_name = "Alibaba-NLP/gte-Qwen2-1.5B-instruct"

    è®¾å®šå›½å†…é•œåƒç„¶åä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°çš„å‘½ä»¤ï¼š
    pip install huggingface_hub
    set HF_ENDPOINT=https://hf-mirror.com
    huggingface-cli download Alibaba-NLP/gte-Qwen2-1.5B-instruct --local-dir ./gte1_5b --local-dir-use-symlinks False
    """

    # embeddings = HuggingFaceEmbeddings(
    #     model_name="./gte1_5b",
    #     model_kwargs={
    #         "device": "cpu", # å¦‚æœæœ‰GPU, å¯ä»¥åŠ é€Ÿï¼Œä½†æ˜¯WindowsåŸç”Ÿç¯å¢ƒè£…ä¸äº† flash_attnï¼ŒCUDAä¼šæŠ¥é”™
    #         "trust_remote_code": True # åƒé—®æ¨¡å‹é‡Œæœ‰è‡ªå®šä¹‰ä»£ç ï¼Œéœ€è¦åŠ è¿™ä¸ªå‚æ•°æˆæƒæ‰èƒ½æ²¡æœ‰æŠ¥é”™
    #     },
    #     encode_kwargs={"normalize_embeddings": True}  # æ¨èå½’ä¸€åŒ–ï¼ŒæŠŠæ¯ä¸ªembeddingå‘é‡æ‹‰åˆ°ç›¸åŒé•¿åº¦ï¼Œå¦åˆ™æ£€ç´¢æ—¶å‘é‡é•¿çš„å¾—åˆ†ä¼šåé«˜
    # )

    """
    æ”¹ç”¨ Ollama çš„ä¸­æ–‡ embedding æ¨¡å‹
    """
    embeddings = OllamaEmbeddings(model="EntropyYue/jina-embeddings-v2-base-zh")

    # æµ‹è¯•å‘é‡ç»´åº¦
    # vec = embeddings.embed_query("ä½ å¥½")
    # print("å‘é‡ç»´åº¦:", len(vec))

    # 3. å†™å…¥å‘é‡æ•°æ®åº“
    vectorstore = PineconeVectorStore(
        index=index,
        embedding=embeddings,
        text_key="page_content"
    )

    vectorstore.add_documents(chunks, batch_size=64) # åˆ†æ‰¹å†™å…¥ï¼Œé¿å…ä¸€æ¬¡æ€§æ•°æ®é‡è¿‡å¤§å¤ªæ…¢

    print("ğŸ‰ Embedding + Pinecone ingest å®Œæˆï¼")



if __name__ == "__main__":
    embedding()